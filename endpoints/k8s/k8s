#!/usr/bin/env bash
# -*- mode: sh; indent-tabs-mode: nil; sh-basic-offset: 4 -*-
# vim: autoindent tabstop=4 shiftwidth=4 expandtab softtabstop=4 filetype=bash

# This script implements the 'k8s' endpoint for rickshaw.  It runs 1 or more
# clients (and no servers, yet) for as many benchmark interations/samples as required
# for a single invocation of rickshaw.  Like all enpoints, this is meant to be used
# with other endpoints for creating multi-cloud tests.
#
# Usage:
# local [--validate] --endpoint-opts=host=<host>,user=<user>,client:n-m,o-p,server:n-m,o-p 
#                    --run-id <id> --base-run-dir --image <location>
#                    --roadblock-server <host> --roadblock-id <id> --roadblock-passwd=<passwd>
#
# If --validate is used all options after client/server will be ignored
#
# TODO: This endpoint script was copied from 'local' endpoint and has some common function.
#       Some of that code could probably be consolidated into a library or utilities.
#       Utilities may work better since other endpoint may be written in a different language.
# TODO: There are a significant number of features we need to implement for pod preferences:
#       - node placement for pods/containers
#       - multus support
#       - pod requests and limits
#       - running benchmark servers and setting up the service and router
#       - containers/pod options
#       - implement tool execution outside the client/server pods (on workers and masters with
#         privileged pods)
#       - collecting information of pod creation and execution
#       - implementing a system info collection of a k8s cluster
#       - using a user-definable container registry for container image sourcing
#         - this is hard-coded right now
#         - rickshaw needs changes to publish locally built images (from workshop) to the user's
#           registry
#         - all endpoint scripts will need to make this change when rickshaw supports this
#
# All of the functions that a engine-script performs works in this endpoint, such as:
# - benchmark iteration/sample execution
# - using a container image which has the required software
# - tool execution
# - synchronization of execution
# - sending tool and benchmark data back to the controller (where this endpoint script runs)

# Source the base file for common functions and config
this_endpoint_dir=$(dirname `readlink -e $0` | sed -e 'sX/binXX')
endpoint_base_dir=$(cd $this_endpoint_dir >/dev/null && cd .. && /bin/pwd)
if [ -e "$endpoint_base_dir/base" ]; then
    . "$endpoint_base_dir/base"
else
    echo "Could not find endpoint source file "$endpoint_base_dir/base", exiting"
    exit 1
    exit
fi
endpoint_name="k8s"
# TODO: instead of using a prefix in the pods' names, use a unique k8s project
pod_prefix="rickshaw"
project_name="crucible-rickshaw"
hypervisor_host="none"
unique_project="0"
hostNetwork="0"
hugepage="0"
osruntime[default]="pod"
runtimeClassName=""
masters_tool_collect="1"
workers_tool_collect="1"
kubeconfig="1"
declare -A cpuPartitioning
declare -A nodeSelector
declare -A PsecurityContext
declare -A securityContext
declare -A resources
declare -A annotations
declare -A volumes
declare -A volumeMounts
declare -A numaNode
declare -A pod_containers

function cleanup_json() {
    local json="$1"; shift

    echo "cleanup_json: processing ${json}"
    mv ${json} ${json}.tmp
    if jq . ${json}.tmp > ${json}; then
        rm ${json}.tmp
    else
        abort_error "cleanup_json failed to process ${json}" endpoint-deploy-begin
    fi
}

function endpoint_k8s_engine_init() {
    echo "Running endpoint_engine_init"
    msg_file="$endpoint_run_dir/env-vars.json"
    echo '[' >$msg_file
    local count=0
    for this_cs_label in $all_pods; do

        hosted_by=`jq -r .spec.nodeName  $endpoint_run_dir/kubectl-get-pod-$this_cs_label.json`

        if [ $count -gt 0 ]; then
            printf "," >>$msg_file
        fi
        echo '{"recipient":{"type":"follower","id":"'$this_cs_label'"},"user-object":{"env-vars":{' >>$msg_file
        echo '"endpoint_label": "'$endpoint_label'",' >>$msg_file
        echo '"hosted_by": "'$hosted_by'",' >>$msg_file
        echo '"hypervisor_host": "'$hypervisor_host'",' >>$msg_file
        echo '"userenv": "'$userenv'",' >>$msg_file
        echo '"osruntime": "'$os_runtime'"' >>$msg_file
        echo '}}}' >>$msg_file

        let count=$count+1
    done
    echo ']' >>$msg_file
}

function endpoint_k8s_test_stop() {
    local msgs_dir="$1"; shift
    local test_id="$1"; shift
    echo "Running endpoint_k8s_test_stop"

    echo "Endpoints:"
    do_ssh $user@$host kubectl -n $project_name get endpoints -o yaml
    endpoints=`do_ssh $user@$host kubectl -n $project_name get endpoints | grep rickshaw | awk '{print $1}'`
    echo "Endpoints to delete:"
    echo $endpoints
    do_ssh $user@$host kubectl -n $project_name delete endpoints $endpoints

    echo "Services:"
    do_ssh $user@$host kubectl -n $project_name get svc -o yaml
    services=`do_ssh $user@$host kubectl -n $project_name get svc | grep rickshaw | awk '{print $1}'`
    echo "Services to delete:"
    echo $services
    if [[ ! -z "$services" ]]; then
       do_ssh $user@$host kubectl -n $project_name delete svc $services
    fi
}

function endpoint_k8s_test_start() {
    # This function runs right after a server starts any service and right before a client starts
    # and tries to contect the server's service.  The purpose of this function is to do any
    # work which ensures the client can contact the server.  In some cases there may be nothing
    # to do.  Regardless of the work, the endpoint needs to relay what IP & ports the client
    # needs to use in order to reach the server.  In some cases that may be the information the
    # server has provided to the endpoint, or this information has changed because the endpoint
    # created some sort of proxy to reach the server.
    #
    # In the case of the k8s endpoint, there are two possible actions, and this depends on where
    # the client is in relation to the server.  If the client is within the same k8s cluster,
    # we create a k8s-service, so the client can use an IP which is more persistent
    # than a pod's IP (this allows pods to come and go while keeping the same IP).  This is not
    # absolutely necessary for our  benchmarks, but it is a best practice for cloud-native
    # aps, so we do it anyway.  If the client is not in the k8s cluster, then we must assume 
    # it does not have direct access to the pod cluster network, and some form of 'ingress' must
    # be set up.  Currently, this endpoint implements 'NodePort', which provides a port for
    # the service which can be accessed on any of the cluster's nodes.  However, we provide the
    # IP address of the node which happens to host the server pod.
    local msgs_dir="$1"; shift
    local test_id="$1"; shift
    local tx_msgs_dir="$1"; shift
    echo "Running endpoint_k8s_test_start"
    if [ "$hostNetwork" != "0"  ]; then
        echo "hostNetwork is enabled. Skip svc creation"
        return
    fi
    # Creating any service or ingress only works if any servers provided information about its
    # IP and ports.
    local this_msg_file="${msgs_dir}/${test_id}:server-start-end.json"
    if [ -e $this_msg_file ]; then
        echo "Found $this_msg_file"
        # Extract the cs-label (server-n, client-y) the IP, and the ports this benchmark is using:
        # server-1 1.2.3.4 30002 30003
        cat $this_msg_file | jq -r '.received[] | if .payload.message.command == "user-object" and .payload.message."user-object".svc.ports then [ .payload.sender.id, .payload.message."user-object".svc.ip, .payload.message."user-object".svc.ports ] | flatten | tostring   else null end' | grep -v null | sed -e 's/"//g' -e 's/\[//' -e 's/\]//' -e 's/,/ /g' >"$endpoint_run_dir/ports.txt"
        while read -u 9 line; do
            # It is possible the server is sending IP/port info that is not for the
            # default network interface, like SRIOV.  In this case, an endpoint does
            # not need to be set up, and the orginial IP/port ifo should be forwarded.
            # TODO: compare IP in message to pod IP, and if not a match pass originial
            # IP/port info in message to client
            echo "line: $line"
            local name=`echo $line | awk '{print $1}'`
            line=`echo $line | sed -e s/^$name//`
            local ip=`echo $line | awk '{print $1}'`
            line=`echo $line | sed -e s/^$ip//`
            #local ports=`echo $line | sed -e s/^$name//`
            local pod_ip=`do_ssh $user@$host "kubectl -n $project_name get pod/rickshaw-$name -o wide" | grep rickshaw | awk '{print $6}'`
            local ports="$line"
            local count=1
            local port_list=""
            for port in $ports; do
                if [ $count -gt 1 ]; then
                    port_list+=", $port"
                else
                    port_list="$port"
                fi
                let count=$count+1
            done
            # Now that we have both the pod default network IP and the IP the server provided, we can figure out
            # if the server-provided IP is the default network, and if so, it is fine to create a k8s-service and
            # k8s-endpoint.
            if [ "$ip" == "$pod_ip" ]; then
                local client_outside_cluster=0
                local client_label=`echo $name | sed -e s/server/client/`
                do_ssh $user@$host "kubectl -n $project_name get pods | grep -q ^rickshaw-$client_label" || client_outside_cluster=1
                # If the client is hosted in this cluster, a clusterIP service will be created
                # for the server, and an endpoint will be created to ensure the service forwards
                # connections to exactly the pod we want.
                #
                if [ "$client_outside_cluster" -ne 1 ]; then
                    local svc_json=$endpoint_run_dir/$name-svc.json
                    echo name: $name
                    echo ports: $ports
                    echo '{' >$svc_json
                    echo '    "apiVersion": "v1",' >>$svc_json
                    echo '    "kind": "Service",' >>$svc_json
                    echo '    "metadata": {' >>$svc_json
                    echo '        "name": "rickshaw-'$name'",' >>$svc_json
                    echo '        "namespace": "'$project_name'"' >>$svc_json
                    echo '    },' >>$svc_json
                    echo '    "spec": {' >>$svc_json
                    echo '        "ports": [' >>$svc_json
                    local next=0
                    for port in $ports; do
                        for proto in TCP UDP; do
                            if [ $next -eq 1 ]; then
                                echo '            ,{' >>$svc_json
                            else
                                echo '            {' >>$svc_json
                                let next=1
                            fi
                            lcproto=`echo $proto | tr [A-Z] [a-z]`
                            echo '                "name": "port-'$port'-'$lcproto'",' >>$svc_json
                            echo '                "port": '$port',' >>$svc_json
                            echo '                "protocol": "'$proto'",' >>$svc_json
                            echo '                "targetPort": '$port >>$svc_json
                            echo '            }' >>$svc_json
                        done
                    done
                    echo '        ]' >>$svc_json
                    echo '    }' >>$svc_json
                    echo '}' >>$svc_json
                    cleanup_json "${svc_json}"
                    cat "$svc_json" | do_ssh $user@$host "kubectl create -f -" >"$endpoint_run_dir/create-svc-$name.txt"
                    # Debug info
                    #do_ssh $user@$host "kubectl get svc/rickshaw-$name -o json" >"$endpoint_run_dir/get-svc-$name.json"
                    # Now that the service has been created, we can get the IP to contact the benchmark server
                    local svc_ip=`do_ssh $user@$host "kubectl -n $project_name get svc/rickshaw-$name -o json" | jq -r .spec.clusterIP`
                    # Instead of relying on k8s to make an association between the service and the pod, we explicitly
                    # connect the two by creating the endpoint, linking the service to the IP of the server pod
                    local svc_endp_json=$endpoint_run_dir/$name-endpoint.json
                    # While the server message does provide the IP, we just use the information we have from k8s
                    echo '{' >$svc_endp_json
                    echo '    "apiVersion": "v1",' >>$svc_endp_json
                    echo '    "kind": "Endpoints",' >>$svc_endp_json
                    echo '    "metadata": {' >>$svc_endp_json
                    echo '        "name": "rickshaw-'$name'",' >>$svc_endp_json
                    echo '        "namespace": "'$project_name'"' >>$svc_endp_json
                    echo '    },' >>$svc_endp_json
                    echo '    "subsets": [{' >>$svc_endp_json
                    echo '        "addresses": [ { "ip": "'$pod_ip'" } ],' >>$svc_endp_json
                    echo '        "ports": [' >>$svc_endp_json
                    local next=0
                    for port in $ports; do
                        for proto in TCP UDP; do
                            if [ $next -eq 1 ]; then
                                echo '            ,{' >>$svc_endp_json
                            else
                                echo '            {' >>$svc_endp_json
                                let next=1
                            fi
                            lcproto=`echo $proto | tr [A-Z] [a-z]`
                            echo '                "name": "port-'$port'-'$lcproto'", "protocol": "'$proto'", "port": '$port'}' >>$svc_endp_json
                        done
                    done
                    echo '        ]' >>$svc_endp_json
                    echo '    }]' >>$svc_endp_json
                    echo '}' >>$svc_endp_json
                    cleanup_json "${svc_endp_json}"
                    cat "$svc_endp_json" | do_ssh $user@$host "kubectl create -f -" >"$endpoint_run_dir/create-svc-endp-$name.txt"
                    # client inside cluster
                else
                    # client outside cluster
                    echo "Client is outside cluster, so creating ingress NodePort Service"
                    # We currently support a "NodePort" type of service
                    local nodep_json=$endpoint_run_dir/$name-nodep.json
                    echo name: $name
                    echo ports: $ports
                    echo '{' >$nodep_json
                    echo '    "apiVersion": "v1",' >>$nodep_json
                    echo '    "kind": "Service",' >>$nodep_json
                    echo '    "metadata": {' >>$nodep_json
                    echo '        "name": "rickshaw-'$name'-nodeport",' >>$nodep_json
                    echo '        "namespace": "'$project_name'"' >>$nodep_json
                    echo '    },' >>$nodep_json
                    echo '    "spec": {' >>$nodep_json
                    echo '        "type": "NodePort",' >>$nodep_json
                    echo '        "ports": [' >>$nodep_json
                    local next=0
                    local port_list=""
                    for port in $ports; do
                        if [ $next -eq 1 ]; then
                            port_list+=", $port"
                            echo '            ,{' >>$nodep_json
                        else
                            port_list="$port"
                            echo '            {' >>$nodep_json
                            let next=1
                        fi
                        echo '                "name": "tcp-port-'$port'",' >>$nodep_json
                        echo '                "nodePort": '$port',' >>$nodep_json
                        echo '                "port": '$port',' >>$nodep_json
                        echo '                "protocol": "TCP",' >>$nodep_json
                        echo '                "targetPort": '$port >>$nodep_json
                        echo '            }' >>$nodep_json
                    done

                    for port in $ports; do
                        echo '            ,{' >>$nodep_json
                        echo '                "name": "udp-port-'$port'",' >>$nodep_json
                        echo '                "nodePort": '$port',' >>$nodep_json
                        echo '                "port": '$port',' >>$nodep_json
                        echo '                "protocol": "UDP",' >>$nodep_json
                        echo '                "targetPort": '$port >>$nodep_json
                        echo '            }' >>$nodep_json
                    done
                    echo '        ]' >>$nodep_json
                    echo '    }' >>$nodep_json
                    echo '}' >>$nodep_json

                    cleanup_json "${nodep_json}"
                    cat "$nodep_json" | do_ssh $user@$host "kubectl create -f -" >"$endpoint_run_dir/create-svc-nodeport-$name.txt"
                    local endp_nodep_json=$endpoint_run_dir/$name-nodeport-endpoint.json
                    echo '{' >$endp_nodep_json
                    echo '    "apiVersion": "v1",' >>$endp_nodep_json
                    echo '    "kind": "Endpoints",' >>$endp_nodep_json
                    echo '    "metadata": {' >>$endp_nodep_json
                    echo '        "name": "rickshaw-'$name'-nodeport",' >>$endp_nodep_json
                    echo '        "namespace": "'$project_name'"' >>$endp_nodep_json
                    echo '    },' >>$endp_nodep_json
                    echo '    "subsets": [{' >>$endp_nodep_json
                    # We use pod's IP
                    echo '        "addresses": [ { "ip": "'$pod_ip'" } ],' >>$endp_nodep_json
                    echo '        "ports": [' >>$endp_nodep_json
                    local next=0
                    for port in $ports; do
                        if [ $next -eq 1 ]; then
                            echo '            ,{' >>$endp_nodep_json
                        else
                            echo '            {' >>$endp_nodep_json
                            let next=1
                        fi
                            echo '                "name": "tcp-port-'$port'", "protocol": "TCP", "port": '$port'}' >>$endp_nodep_json
                    done
                    for port in $ports; do
                            echo '            ,{' >>$endp_nodep_json
                            echo '                "name": "udp-port-'$port'", "protocol": "UDP", "port": '$port'}' >>$endp_nodep_json
                    done
                    echo '        ]' >>$endp_nodep_json
                    echo '    }]' >>$endp_nodep_json
                    echo '}' >>$endp_nodep_json
                    cleanup_json "${endp_nodep_json}"
                    cat "$endp_nodep_json" | do_ssh $user@$host "kubectl create -f -" >"$endpoint_run_dir/create-endp-nodeport-$name.txt"
                    # $svc_ip must now be reassigned to the IP used for NodePort.  NodePort is available on -any- worker node
                    # However, we should provide an IP of the current worker which hosts the pod
                    local node=`do_ssh $user@$host "kubectl -n $project_name get pod rickshaw-$name -o json" | jq -r '.spec.nodeName'`
                    echo "Finding IP for worker node $node"
                    svc_ip=`do_ssh $user@$host "kubectl get nodes/$node -o wide" | grep $node | awk '{print $6}' | tr -d "\n"`
                fi # client is outside cluster
            else
                echo "Benchmark-server-provided IP $ip does not match pod IP $pod_ip, so not creating a k8s-service or k8s-endpoint"
                svc_ip=$ip
            fi
            # Now we can consruct a message to be sent to the client about the IP and ports for the server
            echo -n '{"recipient":{"type":"follower","id":"client-' >"$tx_msgs_dir/service-ip-$name.json"
            echo $name | awk -F- '{print $2}' | tr -d "\n" >>"$tx_msgs_dir/service-ip-$name.json"
            echo -n '"},"user-object":{"svc":{"ip": "'$svc_ip'", ' >>"$tx_msgs_dir/service-ip-$name.json"
            echo '"ports": ['$port_list']}}}' >>"$tx_msgs_dir/service-ip-$name.json"
            echo "endpoint-k8s-start has contstructed this message to send to the client"
            cat "$tx_msgs_dir/service-ip-$name.json"
        done 9< "$endpoint_run_dir/ports.txt"
    else
        echo "Did not find $this_msg_file"
    fi
    sleep 10 # why?
    echo "services:"
    do_ssh $user@$host "kubectl get svc"
    do_ssh $user@$host kubectl -n $project_name get svc -o json >"$endpoint_run_dir/kubectl-get-services.json"
    echo "endpoints:"
    do_ssh $user@$host "kubectl get endpoints"
    do_ssh $user@$host kubectl -n $project_name get endpoints -o json >"$endpoint_run_dir/kubectl-get-endpoints.json"
    echo "pods:"
    do_ssh $user@$host "kubectl get pods -o wide"
    do_ssh $user@$host kubectl -n $project_name get pods -o json >"$endpoint_run_dir/kubectl-get-pods.json"
}

function k8s_req_check() {
    if [ -z "$host" ]; then
        exit_error "k8s host is not defined"
    fi
    verify_ssh_login $user $host
    if [ "${kubeconfig}" == "1" ]; then
        k8s_kubeconfig=`do_ssh $user@$host env | grep ^KUBECONFIG | awk -F"KUBECONFIG=" '{print $2}'`
        if [ -z "$k8s_kubeconfig" ]; then
            exit_error "KUBECONFIG on k8s host $host not defined"
        fi
    fi
    k8s_kubectl=`do_ssh $user@$host kubectl 2>&1`
    if [ $? -gt 0 ]; then
        exit_error "Could not run kubectl on k8s host: $k8s_kubectl"
    fi
    # Validation returns what clients and servers would be used and the userenv
    if [ "$do_validate" == 1 ]; then
        echo_clients_servers
        echo "userenv $userenv"
        exit
    fi
}

function process_k8s_opts() {
    local endpoint_opts="$1"
    for opt in `echo $endpoint_opts | sed -e 's/,/ /g'`; do
        arg=`echo $opt| awk -F: '{print $1}'`
        # The $val may have : in it, so don't use awk to get only the second field
        val=`echo $opt | sed -e s/^$arg://`
        case "$arg" in
            unique-project)
                unique_project=${val}
                ;;
            kubeconfig)
                kubeconfig=$val
                ;;
            client|server|clients|servers)
                addto_clients_servers "$arg" "$val"
                ;;
            host)
                host=$val
                if [ -z "$controller_ipaddr" ]; then
                    controller_ipaddr=`get_controller_ip $host`
                fi
                ;;
            controller-ip)
                controller_ipaddr=$val
                ;;
            user)
                user=$val
                ;;
            userenv)
                userenv=$val
                ;;
            hugepage)
                hugepage=$val
                ;;
            masters-tool-collect)
                masters_tool_collect=$val
                ;;
            workers-tool-collect)
                workers_tool_collect=$val
                ;;
            hostNetwork)
                hostNetwork="$val"
                ;;
            runtimeClassName)
                runtimeClassName="$val"
                ;;
            nodeSelector)
                # nodeSelector is per engine:
                # option format::  nodeSelector:<engine-label>:<full-path-to-json-with-nodeSelector>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                # <engine-label> can be an engine type (client, server) and a '-', followed by range(s) of numbers:
                #   client-1-3+5-7+9-11   <-ranges must be separated with '+'
                # json file example (note no outer {}'s)
                # "nodeSelector": {
                #    "kubernetes.io/hostname": "worker000"
                # }
                #TODO: validate correct format of <engine-label>:<path-to-file>
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                label=`echo $name | awk -F- '{print $1}'`
                range=`echo $name | sed -e s/$label-//`
                if [ ! -z "$file" -o -e $file ]; then
                    if [ "$label" == "default" ]; then
                        nodeSelector["default"]=`cat $file`
                    else
                        local list=`range_to_list $range`
                        for this_id in $list; do
                            nodeSelector[$label-$this_id]=`cat $file`
                        done
                    fi
                else
                    exit_error "Could not find nodeSelector file $file for $name"
                fi
                ;;
            securityContext)
                # securityContext is per engine:
                # option format::  securityContext:<engine-label>:<full-path-to-json-with-securityContext>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                # <engine-label> can be an engine type (client, server) and a '-', followed by range(s) of numbers:
                #   client-1-3+5-7+9-11   <-ranges must be separated with '+'
                # json file example (note no outer {}'s)
                # "securityContext": {
                #   "privileged": false,
                #   "capabilities": {
                #     "add": [ "IPC_LOCK", "SYS_ADMIN", "SYS_RAWIO", "SYS_NICE" ]
                #   }
                # }
                #TODO: validate correct format of <engine-label>:<path-to-file>
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                label=`echo $name | awk -F- '{print $1}'`
                range=`echo $name | sed -e s/$label-//`
                if [ ! -z "$file" -o -e $file ]; then
                    if [ "$label" == "default" ]; then
                        securityContext["default"]=`cat $file`
                    else
                        local list=`range_to_list $range`
                        for this_id in $list; do
                            securityContext[$label-$this_id]=`cat $file`
                        done
                    fi
                else
                    exit_error "Could not find securityContext file $file for $name"
                fi
                ;;
            PsecurityContext)
                # PsecurityContext is per engine:
                # option format::  PsecurityContext:<engine-label>:<full-path-to-json-with-securityContext>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                # <engine-label> can be an engine type (client, server) and a '-', followed by range(s) of numbers:
                #   client-1-3+5-7+9-11   <-ranges must be separated with '+'
                # json file example (note no outer {}'s)
                # "securityContext": {
                #       "sysctls": [
                #          {
                #             "name": "net.ipv4.tcp_tw_reuse",
                #             "value": "1"
                #          }
                #       ]
                # }
                #TODO: validate correct format of <engine-label>:<path-to-file>
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                label=`echo $name | awk -F- '{print $1}'`
                range=`echo $name | sed -e s/$label-//`
                if [ ! -z "$file" -o -e $file ]; then
                    if [ "$label" == "default" ]; then
                        PsecurityContext["default"]=`cat $file`
                    else
                        local list=`range_to_list $range`
                        for this_id in $list; do
                            PsecurityContext[$label-$this_id]=`cat $file`
                        done
                    fi
                else
                    exit_error "Could not find PsecurityContext file $file for $name"
                fi
                ;;
            resources)
                # resources is per engine:
                # option format::  resources:<engine-label>:<full-path-to-json-with-resources>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                # <engine-label> can be an engine type (client, server) and a '-', followed by range(s) of numbers:
                #   client-1-3+5-7+9-11   <-ranges must be separated with '+'
                # json file example (note no outer {}'s)
                # "resources": {
                #   "requests": {
                #     "cpu": "33",
                #     "memory": "2048Mi"
                #   },
                #   "limits": {
                #     "cpu": "33",
                #     "memory": "2048Mi"
                #   }
                # }
                #TODO: validate correct format of <engine-label>:<path-to-file>
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                label=`echo $name | awk -F- '{print $1}'`
                range=`echo $name | sed -e s/$label-//`
                if [ ! -z "$file" -o -e $file ]; then
                    if [ "$label" == "default" ]; then
                        resources["default"]=`cat $file`
                    else
                        local list=`range_to_list $range`
                        for this_id in $list; do
                            resources[$label-$this_id]=`cat $file`
                        done
                    fi
                else
                    exit_error "Could not find resources file $file for $name"
                fi
                ;;
            annotations)
                # resources is per engine:
                # option format::  annotations:<engine-label>:<full-path-to-json-with-annotations>
                # <engine-label> can be 'default' to apply to any engine that is not explicitly specified
                # <engine-label> can be an engine type (client, server) and a '-', followed by range(s) of numbers:
                #   client-1-3+5-7+9-11   <-ranges must be separated with '+'
                # json file example (note no outer {}'s)
                # "annotations": {
                #   "k8s.v1.cni.cncf.io/networks": "sriov-mellanox-b-pod, sriov-mellanox-a-pod"
                # }
                #TODO: validate correct format of <engine-label>:<path-to-file>
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                label=`echo $name | awk -F- '{print $1}'`
                range=`echo $name | sed -e s/$label-//`
                if [ ! -z "$file" -o -e $file ]; then
                    if [ "$label" == "default" ]; then
                        annotations["default"]=`cat $file`
                    else
                        local list=`range_to_list $range`
                        for this_id in $list; do
                            annotations[$label-$this_id]=`cat $file`
                        done
                    fi
                else
                    exit_error "Could not find annotations file $file for $name"
                fi
                ;;
            volumeMounts)
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                if [ ! -z "$file" -o -e $file ]; then
                    volumeMounts[$name]=`cat $file`
                else
                    exit_error "Could not find volumeMounts file $file for $name"
                fi
                ;;
            volumes)
                name=$(get_opt_field "${val}" "key")
                file=$(get_opt_field "${val}" "value")
                if [ ! -z "$file" -o -e $file ]; then
                    volumes[$name]=`cat $file`
                else
                    exit_error "Could not find volumes file $file for $name"
                fi
                ;;
            *)
                if echo $arg | grep -q -- "="; then
                    echo "You can't use a \"=\" for assignment in endpoint options"
                    echo "You must use \":\", like `echo $arg | sed -e 's/=/:/'`"
                fi
                exit_error "k8s endpoint option [$arg] not supported"
                ;;
        esac
    done

    if [ "${unique_project}" == "1" ]; then
        project_name+="--${run_id}-${endpoint_label}"
    fi
}

function build_pod_spec() {
    local name=$1; shift
    local type=$1; shift
    local dir=$1; shift
    local node=$1; shift
    local json="$dir/$name.json"
    local label=`echo $name | awk -F- '{print $1}'`
    local id=`echo $name | awk -F- '{print $2}'`
    local this_image=""
    local containers

    if [ "$type" == "cs" ]; then
        set_osruntime_numanode_cpupart $name
    fi

    echo "{" >$json
    echo "  \"apiVersion\": \"v1\"" >>$json
    echo "  ,\"kind\": \"Pod\"" >>$json
    echo "  ,\"metadata\": {" >>$json
    echo "    \"name\": \"$pod_prefix-$name\"," >>$json
    echo "    \"namespace\": \"$project_name\"" >>$json
    echo "  }" >>$json
    echo "  ,\"spec\": {" >>$json
    echo "    \"restartPolicy\": \"Never\"" >>$json




    if [ "$type" == "master" ]; then

        # Ensure master pod can be scheduled on master node
        echo '    ,"tolerations": [' >>$json
        echo '        {' >>$json
        echo '            "key": "node-role.kubernetes.io/master",' >>$json
        echo '            "effect": "NoSchedule"' >>$json
        echo '        }' >>$json
        echo '    ]' >>$json
    fi


    if [ "$type" == "worker" -o "$type" == "master" ]; then

        # Worker/master tools need specific node placement
        echo "    ,\"nodeSelector\": {" >>$json
        echo "        \"kubernetes.io/hostname\": \"$node\"" >>$json
        echo "    }" >>$json
        echo "    ,\"hostPID\": true" >>$json
        echo "    ,\"hostNetwork\": true" >>$json
        echo "    ,\"hostIPC\": true" >>$json

        # Worker/master tools need specific filesystems
        echo '       ,"volumes": [' >>$json
        echo '          { "name": "hostfs-run",' >>$json
        echo '            "hostPath": {' >>$json
        echo '              "path": "/var/run",' >>$json
        echo '              "type": "Directory"' >>$json
        echo '            }' >>$json
        echo '          },' >>$json
        echo '          { "name": "hostfs-firmware",' >>$json
        echo '            "hostPath": {' >>$json
        echo '              "path": "/lib/firmware",' >>$json
        echo '              "type": "Directory"' >>$json
        echo '            }' >>$json
        echo '          },' >>$json
        echo '          { "name": "hostfs-modules",' >>$json
        echo '            "hostPath": {' >>$json
        echo '              "path": "/lib/modules",' >>$json
        echo '              "type": "Directory"' >>$json
        echo '            }' >>$json
        echo '          }' >>$json
        echo '        ]' >>$json

    fi


    if [ "$type" == "cs" ]; then

        # Client/server user-provided resources
        set +u
        if [ ! -z "${resources[$name]}" ]; then
            echo -e ",${resources[$name]}" >>$json
        elif [ ! -z "${resources[default]}" ]; then
            echo -e ",${resources[default]}" >>$json
        fi
        set -u

        # Support override of runtimeClassName
        # This can be used for Perf Addon Profiles
        if [ ! -z "$runtimeClassName" ]; then
            echo "    ,\"runtimeClassName\": \"$runtimeClassName\"" >>$json
        elif [ "$os_runtime" == "kata" ]; then
            echo "    ,\"runtimeClassName\": \"kata\"" >>$json
        fi

        # Client/server user-provided node selector
        set +u
        if [ ! -z "${nodeSelector[$name]}" ]; then
            echo -e "    ,${nodeSelector[$name]}" >>$json
        elif [ ! -z "${nodeSelector[default]}" ]; then
            echo -e "    ,${nodeSelector[default]}" >>$json
        fi
        set -u

        # Client/server opt in to host network
        if [ ! -z "$hostNetwork" -a "$hostNetwork" == "1" ]; then
            echo "    ,\"hostNetwork\": true" >>$json
        fi

        # Client/server user-provided volumes
        set +u
        if [ ! -z "${volumes[$name]}" ]; then
            echo -e ",${volumes[$name]}" >>$json
        else
            # Default volumes
            echo '       ,"volumes": [' >>$json
            echo '          { "name": "hostfs-firmware",' >>$json
            echo '            "hostPath": {' >>$json
            echo '              "path": "/lib/firmware",' >>$json
            echo '              "type": "Directory"' >>$json
            echo '            }' >>$json
            echo '          }' >>$json
            if [ "$hugepage" == "1" ]; then
                echo '          ,{ "name": "hugepage",' >>$json
                echo '            "emptyDir": {' >>$json
                echo '              "medium": "HugePages"' >>$json
                echo '            }' >>$json
                echo '          }' >>$json
            fi
            echo '        ]' >>$json
        fi
        set -u

    fi

    # Each crucible engine is one container
    # One tool per engine
    echo '    ,"containers": [' >>$json
    new_profiler_engines=""
    if [ "$type" == "worker" -o "$type" = "master" ]; then
        containers=""
        # The scope for workers and master in the call to
        # add_profiler_engines() does not include an id because
        # tool cmds do no differ between different worker/master pods
        add_profiler_engines $type new_profiler_engines
    else
        containers="$name"
        add_profiler_engines $name new_profiler_engines
    fi
    containers+=" $new_profiler_engines"
    container_count=1

    echo "containers to be created: $containers"
    for this_container in $containers; do

        if [ "$container_count" -eq 1 ]; then
            pod_containers[$name]=$this_container
        else
            pod_containers[$name]+=" $this_container"
            echo "," >>$json
        fi
        echo "      {" >>$json # Start of this container
        echo "        \"name\": \"$this_container\"," >>$json

        # Image is specific to the benchmark or individual tool
        get_image_name "$this_container" this_image
        if [ -z "$this_image" ]; then
            abort_error "Could not find image for $name" endpoint-deploy-begin
        fi

        echo "        \"image\": \"$this_image\"," >>$json
        echo "        \"imagePullPolicy\": \"Always\"," >>$json
        echo "        \"env\": [" >>$json
        echo "          {" >>$json
        echo "            \"name\": \"rickshaw_host\"," >>$json
        echo "            \"value\": \"$controller_ipaddr\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"base_run_dir\"," >>$json
        echo "            \"value\": \"$base_run_dir\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"cs_label\"," >>$json
        echo "            \"value\": \"$this_container\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"endpoint_run_dir\"," >>$json
        echo "            \"value\": \"/endpoint-run\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"endpoint\"," >>$json
        echo "            \"value\": \"k8s\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"osruntime\"," >>$json
        echo "            \"value\": \"$os_runtime\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"roadblock_passwd\"," >>$json
        echo "            \"value\": \"flubber\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"roadblock_id\"," >>$json
        echo "            \"value\": \"$rb_id\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"max_sample_failures\"," >>$json
        echo "            \"value\": \"$max_sample_failures\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"max_rb_attempts\"," >>$json
        echo "            \"value\": \"$max_rb_attempts\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"engine_script_start_timeout\"," >>$json
        echo "            \"value\": \"$engine_script_start_timeout\"" >>$json
        echo "          }," >>$json
        echo "          {" >>$json
        echo "            \"name\": \"ssh_id\"," >>$json
        printf "            \"value\": \"" >>$json
        sed -z 's/\n/\\n/g' $config_dir/rickshaw_id.rsa >>$json
        echo "\"" >>$json
        echo "      }" >>$json
        if [ "$type" == "cs" ]; then
            echo "          ,{" >>$json
            echo "            \"name\": \"cpu_partitioning\"," >>$json
            set +u
            if [ ! -z "${cpuPartitioning[$name]}" ]; then
                echo -e "            \"value\": \"${cpuPartitioning[$name]}\"" >>$json
            elif [ ! -z "${cpuPartitioning[default]}" ]; then
                echo -e "            \"value\": \"${cpuPartitioning[default]}\"" >>$json
            else
                echo -e "            \"value\": \"0\"" >>$json
            fi
            set -u
            echo "          }" >>$json
        fi
        echo "    ]" >>$json # End of env[]


        if [ "$type" == "worker" -o "$type" == "master" ]; then

            # Worker/mastr tools need privileged security
            echo "        ,\"securityContext\": {" >>$json
            echo "          \"privileged\": true" >>$json
            echo "        }" >>$json

            echo '       ,"volumeMounts": [' >>$json
            echo '          { "mountPath": "/var/run",' >>$json
            echo '            "name": "hostfs-run"' >>$json
            echo '          },' >>$json
            echo '          { "mountPath": "/lib/firmware",' >>$json
            echo '            "name": "hostfs-firmware"' >>$json
            echo '          },' >>$json
            echo '          { "mountPath": "/lib/modules",' >>$json
            echo '            "name": "hostfs-modules"' >>$json
            echo '          }' >>$json
            echo "        ]" >>$json
        fi


        # If user provided volumeMounts.json, use it. It not, and "hugepage", do implicitly
        if [ "$type" == "cs" ]; then

            # Client/server user-provided security context
            set +u
            if [ ! -z "${securityContext[$name]}" ]; then
                echo -e ",${securityContext[$name]}" >>$json
            elif [ ! -z "${securityContext[default]}" ]; then
                echo -e ",${securityContext[default]}" >>$json
            fi
            set -u

            set +u
            if [ ! -z "${volumeMounts[$name]}" ]; then
                echo -e ",${volumeMounts[$name]}" >>$json
            else
                echo '       ,"volumeMounts": [' >>$json
                echo '          { "mountPath": "/lib/firmware",' >>$json
                echo '            "name": "hostfs-firmware"' >>$json
                echo '          }' >>$json
                if [ "$hugepage" == "1" ]; then
                    echo '          ,{ "mountPath": "/dev/hugepages",' >>$json
                    echo '            "name": "hugepage"' >>$json
                    echo '          }' >>$json
                fi
                echo "        ]" >>$json
            fi
            set -u
        fi


        echo "      }" >>$json # End of this container{}

        let container_count=$container_count+1
    done
    echo "    ]" >>$json # End of containers[]

    echo "  }" >>$json # end of spec{}
    echo "}" >>$json # end of {}
    cleanup_json "${json}"
}

function create_pods() {
    typeset -n ref1=$1; shift # caller-provided variable name (call-by-reference)
    ref1=""
    local type="$1"; shift
    # remaining args are engine labels or worker/master node names
    local dir="$endpoint_run_dir/$type-pod-objects"
    local pods=""
    if [ -z "$*" ]; then
        if [ "$type" == "master" -o "$type" == "worker" ]; then
            echo "The list of nodes for tool-pods for type $type is empty"
            echo "Something may be wrong in detecting these nodes"
        else
            abort_error "You must request one or more pods to create" endpoint-deploy-begin
        fi
    fi
    mkdir -p "$dir"
    local count=1
    declare -A pod_to_node # Keep a list of pods to create with optional node to place it
    while [ $# -gt 0 ]; do
        local name="$1"; shift
        echo "create_pods: working on [$name]"
        local node=""
        if [ "$type" == "master" -o "$type" == "worker" ]; then
            node="$name"
            echo "because this is a tool pod, changing name from $name to..."
            name="$type-$count"
            echo "$name"
            let count=$count+1
            pod_to_node[$name]=$node
        else
            # Non-tool pods don't get host placement here
            pod_to_node[$name]=""
        fi
    done
    delete_pods ${!pod_to_node[@]}
    local this_pod
    for this_pod in ${!pod_to_node[@]}; do
        echo build_pod_spec "$this_pod" "$type" "$dir" "${pod_to_node[$this_pod]}"
        build_pod_spec "$this_pod" "$type" "$dir" "${pod_to_node[$this_pod]}"
        if [ -e "$dir/$this_pod.json" ]; then
            echo "user: $user"
            echo "host: $host"
            # TODO: the exit code does not work here, need to find a way to have it work
            cat "$dir/$this_pod.json" | do_ssh $user@$host "kubectl create -f - 2>&1" >"$endpoint_run_dir/create-pod-output-$this_pod.txt"
            if [ $? -gt 0 ]; then
                abort_error "Failed to create pod $this_pod" endpoint-deploy-begin
            fi
            pods+=" $this_pod"
            do_ssh $user@$host "kubectl  -n $project_name get pod rickshaw-$this_pod -ojson" >$endpoint_run_dir/kubectl-get-pod-$this_pod.json
        else
            abort_error "Could not find $dir/$this_pod.json to create pod: $create_output" endpoint-deploy-begin
        fi
    done
    do_ssh $user@$host kubectl -n $project_name get pods 2>&1 >"$endpoint_run_dir/post-create-get-pods.txt"
    ref1="$pods"
}

function verify_pods_running() {
    typeset -n ref1=$1; shift # caller-provided variable name (call-by-reference)
    ref1=""
    local kubectl_get_pods="$endpoint_run_dir/kubectl-get-pods.txt"
    declare -A unverified_pods=()
    declare -A verified_pods=()
    if [ -z "$1" ]; then
        abort_error "You must provide at least 1 pod to verify" endpoint-deploy-begin
    fi
    while [ $# -gt 0 ]; do
        unverified_pods[$1]=1; shift
    done
    local num_pods=${#unverified_pods[@]}
    local count=0
    local max_attempts=25
    local abort=0
    local num_nonzero_exit=0
    local max_nonzero_exit=3
    until [ ${#unverified_pods[@]} -eq 0 -o $count -gt $max_attempts -o $abort -gt 0 ]; do
        sleep 10
        do_ssh $user@$host kubectl -n $project_name get pods -o wide | grep $pod_prefix >$kubectl_get_pods
        rc=$?
        if [ $rc -gt 0 ]; then
            let num_nonzero_exit=$num_nonzero_exit+1
            if [ $num_nonzero_exit -gt $max_nonzero_exit ]; then
                abort_error "kubectl-get-pods returns non-zero more than $max_nonzero_exit times" endpoint-deploy-begin
                return
            fi
        fi
        echo "kubectl get pods:"
        cat $kubectl_get_pods
        while read line; do
            echo "got this line: $line"
            local this_status=`echo $line | awk '{print $3}'`
            local this_pod=`echo $line | awk '{print $1}' | sed -e s/^$pod_prefix-//`
            local node=`echo $line | awk '{print $7}'`
            if echo "$this_status" | grep -q -i error; then
                # If just 1 pod is in error bail immediately
                abort=1
                echo "Pod $this_pod has this error: $this_status"
                echo "Getting 'describe pod' for $pod_prefix-$this_pod"
                do_ssh $user@$host kubectl -n $project_name describe pod $pod_prefix-$this_pod >"$endpoint_run_dir/kubectl-describe-pod-$this_pod.txt"
                break
            fi
            if [ "$this_status" == "Running" ]; then
                verified_pods[$this_pod]="$node"
                unset unverified_pods[$this_pod]
            fi
        done <$kubectl_get_pods
        let count=$count+1
    done
    if [ $abort -eq 1 -o $count -gt $max_attempts ]; then
        #TODO: send abort signal for endpoint-deploy roadblock
        echo "abort: $abort count: $count"
        abort_error "Failed to verify pods are running" endpoint-deploy-begin
    fi
    declare -A nodes
    for i in ${verified_pods[@]}; do
        nodes[$i]=1
    done
    # var is assigned a space-separated list of nodes which host these pods
    ref1=`echo "${!nodes[@]}" | sed -e 's;[ |,];\n;g' | sort | xargs`
}

function get_pod_logs() {
    local name pod_name total_rc

    total_rc=0

    if [ -z "$1" ]; then
        abort_error "get_pod_logs(): at least 1 pod must be provided" endpoint-deploy-begin
    fi

    echo "Collecting pod logs:"

    while [ $# -gt 0 ]; do
        local name="$1"; shift
        echo "Getting logs from pod ${name}"
        pod_name="rickshaw-${name}"
        for this_container in ${pod_containers[$name]}; do
            do_ssh ${user}@${host} kubectl -n ${project_name} logs ${pod_name} >"${engine_logs_dir}/${name}-${this_container}.txt"
            (( total_rc += ${?} ))
        done
    done

    return ${total_rc}
}

function delete_pods() {
    if [ -z "$1" ]; then
        abort_error "delete_pods(): at least 1 pod must be provided" endpoint-deploy-begin
    fi
    local pods_to_delete=""
    while [ $# -gt 0 ]; do
        local name="$1"; shift
        echo "checking for existing pod $name with prefix $pod_prefix-"
        local existing_pod="`do_ssh $user@$host kubectl -n $project_name get pods | grep "^$pod_prefix-$name " | awk '{print $1}'`"
        if [ ! -z "$existing_pod" ]; then
            echo "delete_pods(): found $name"
            pods_to_delete+=" $pod_prefix-$name"
        fi
    done
    if [ ! -z "$pods_to_delete" ]; then
        echo "going to bulk-delete these pods: $pods_to_delete"
        local delete_output=`do_ssh $user@$host "kubectl -n $project_name delete pod $pods_to_delete 2>&1"`
        if [ $? -gt 0 ]; then
            abort_error "Error deleting pod $existing_pod: $delete_output" endpoint-deploy-begin
        fi
    fi
}

function delete_pods_namespace() {
    local delete_rc

    do_ssh ${user}@${host} kubectl -n ${project_name} delete pods --all
    delete_rc=${?}

    return ${delete_rc}
}

function delete_old_pods() {
    local pods=`do_ssh $user@$host kubectl -n $project_name get pods -o wide | grep $pod_prefix | awk '{print $1}' | sed -e s/$pod_prefix-// | tr '\n' ' '`
    if [ ! -z "$pods" ]; then
        echo "Going to delete these old pods: $pods"
        delete_pods $pods
    else
        echo "No old pods to delete"
    fi
}

function get_k8s_config() {
    local kubectl_nodes_json="$endpoint_run_dir/kubectl-get-nodes.json"
    local kubectl_nodes_stderr="$endpoint_run_dir/kubectl-get-nodes.stderr"
    local k8s_nodes=`do_ssh $user@$host kubectl get nodes`
    do_ssh $user@$host kubectl get nodes -o json >$kubectl_nodes_json 2>$kubectl_nodes_stderr
    local nr_nodes=`jq -r '.items | length' $kubectl_nodes_json`
    local masters=""
    local workers=""
    local node=""
    for node in `seq 0 $(expr $nr_nodes - 1)`; do
        local name=`jq -r '.items['$node'] | .metadata.name' $kubectl_nodes_json`
        if [ "$(jq -r '.items['$node'] | .metadata.labels | has("node-role.kubernetes.io/worker")' $kubectl_nodes_json)" == "true" ]; then
            workers="$workers $name"
        fi
        if [ "$(jq -r '.items['$node'] | .metadata.labels | has("node-role.kubernetes.io/master")' $kubectl_nodes_json)" == "true" ]; then
            masters="$masters $name"
        fi
    done
    echo "$workers" >"$endpoint_run_dir/worker-nodes.txt"
    echo "$masters" >"$endpoint_run_dir/master-nodes.txt"
}

function endpoint_k8s_sysinfo() {
    local remote_base_dir remote_dir local_dir
    remote_base_dir="/var/lib/crucible"
    remote_dir="${remote_base_dir}/${endpoint_label}_${run_id}"
    local_dir="${endpoint_run_dir}/sysinfo"

    mkdir ${local_dir}

    do_ssh ${user}@${host} kubectl cluster-info > ${local_dir}/cluster-info.txt
    do_ssh ${user}@${host} kubectl cluster-info dump > ${local_dir}/cluster-info.dump.json

    do_ssh ${user}@${host} oc cluster-info > /dev/null 2>&1
    if [ $? == 0 ]; then
        echo "Detected that K8S is OpenShift"

        do_ssh ${user}@${host} mkdir -p ${remote_dir}/must-gather

        do_ssh ${user}@${host} oc adm must-gather --dest-dir=${remote_dir}/must-gather > ${local_dir}/must-gather.txt 2>&1
        do_scp ${user}@${host} ${remote_dir}/must-gather "" ${local_dir}

        do_ssh ${user}@${host} rm -Rf ${remote_dir}
    else
        echo "Detected that K8S is not OpenShift"
    fi
}

function create_k8s_project() {
    do_ssh $user@$host kubectl create namespace $project_name
}
function endpoint_k8s_cleanup() {
    local log_rc delete_pods_rc delete_namespace_rc

    log_rc=0
    delete_pods_rc=0
    delete_namespace_rc=0

    echo "Current pod state:"
    do_ssh ${user}@${host} kubectl -n ${project_name} get pods -o wide | grep ${pod_prefix}

    get_pod_logs $all_pods
    log_rc=${?}

    if [ ${log_rc} == 0 ]; then
        delete_pods_namespace
        delete_pods_rc=${?}

        if [ ${delete_pods_rc} == 0 ]; then
            do_ssh $user@$host kubectl delete namespace $project_name
            delete_namespace_rc=${?}
        else
            echo "Error deleting namespace pods"
        fi
    else
        echo "Error getting pod logs"
    fi

    return $(( ${log_rc} + ${delete_pods_rc} + ${delete_namespace_rc} ))
}

process_opts "$@"
process_common_endpoint_opts k8s_opts $endpoint_opts
echo "#k8s opts: [$k8s_opts]"
process_k8s_opts $k8s_opts
init_common_dirs
load_settings
k8s_req_check
create_k8s_project
base_req_check
get_k8s_config
delete_pods_namespace
#delete_old_pods

echo "This endpoint to run these clients: ${clients[@]}"
echo "This endpoint to run these servers: ${servers[@]}"
# A subset of pods have cpu-paritioning and therefore should be using static-cpu pod.
# We want those pods created first, so the remaining non-static-cpu pods get a cpus-allowed mask
# that is accurate (after all dedicated cpus are allocated).  If the non-static-cpu pods are
# created before/during all the static-cpu pods, their cpus-allowed mask will change over time.
cpu_part_pods=""
regular_pods=""

# The following tracks pods that have been created
cs_pods=""
worker_tool_pods=""
master_tool_pods=""
all_pods=""

# All roadblock particpants are not determined until it is known
# where tools are run.  Once this is known, this information needs
# to be sent back to the controller.
new_k8s_followers=""

# Split up the cs pods into two groups, ones which
# requested cpu-part, and the ones that did not.
# This is necessary because cpu-part pods need to be
# created first.
for this_pod in ${clients[@]} ${servers[@]}; do
    if [ "${cpuPartitioning[$this_pod]}" == 1 ]; then
        cpu_part_pods+=" $this_pod"
    elif [ "${cpuPartitioning[$this_pod]}" == 0 ]; then
        regular_pods+=" $this_pod"
    elif [ "${cpuPartitioning[default]}" == 1 ]; then
        cpu_part_pods+=" $this_pod"
    elif [ "${cpuPartitioning[default]}" == 0 ]; then
        regular_pods+=" $this_pod"
    else
        regular_pods+=" $this_pod"
    fi
done

# Create the cpu-part pods first
if [ ! -z "$cpu_part_pods" ]; then
    these_pods=""
    create_pods these_pods cs $cpu_part_pods
    echo "These client/server pods with cpu-partitioning feature were created: $these_pods"
    verify_pods_running active_worker_nodes $these_pods
    cs_pods+=" $these_pods"
fi

# Now that cpumask on worker node(s) has been reduced (to dedicate for cpu-part pods).
# the regular pods can be created.
if [ ! -z "$regular_pods" ]; then
    these_pods=""
    create_pods these_pods cs $regular_pods
    echo "These client/server pods without cpu-partioning feature pods were created: $these_pods"
    verify_pods_running active_worker_nodes $these_pods
    cs_pods+=" $these_pods"
fi
echo "Total client/server pods that were created: $cs_pods"
echo "These nodes are hosting the engine pods: $active_worker_nodes"
all_pods+=" $cs_pods"

if [ "${workers_tool_collect}" == "1" ]; then
    echo "Working on creating worker-tool pods"
    create_pods worker_tool_pods worker $active_worker_nodes
    echo "These worker-tool pods were created: $worker_tool_pods"
    verify_pods_running tool_worker_nodes $worker_tool_pods
    echo "These nodes are hosting the worker-tool pods: $tool_worker_nodes"
    new_k8s_followers+=" ${worker_tool_pods}"
    all_pods+=" $worker_tool_pods"
fi

master_nodes=`cat "$endpoint_run_dir/master-nodes.txt"`

echo "These nodes are masters: $master_nodes"
if [ -z "${master_nodes}" ]; then
    masters_tool_collect="0"
fi
if [ "${masters_tool_collect}" == "1" ]; then
    echo "Working on creating these master-tool pods"
    create_pods master_tool_pods master $master_nodes
    echo "These master-tool pods were created: $master_tool_pods"
    verify_pods_running tool_master_nodes $master_tool_pods
    echo "These nodes are hosting the master-tool pods: $master_nodes"
    new_k8s_followers+=" ${master_tool_pods}"
    all_pods+=" $master_tool_pods"
fi

process_roadblocks k8s $new_followers
